{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "698e761f-e6c2-49e8-8253-57420f00a93d",
   "metadata": {},
   "source": [
    "### Project 1. Predicting Walmart Sales\n",
    "\n",
    "Predicting the weekly sales of 45 different Walmart Stores, based on data from 2010 - 2012.\n",
    "The factors considered include:\n",
    "* The average air temperature in the region\n",
    "* The cost of fuel in the region\n",
    "* That week's consumer price index\n",
    "* The unemployment rate in the region\n",
    "* Whether a holiday occured in that week\n",
    "* (engineered) the month the week happened in\n",
    "* (engineered) the week's number within the month\n",
    "\n",
    "The main goal of this project is to predict the weekly sales of any of the Walmart stores considered, using regression models, but, an additional task could involve the prediction of the store based on the sales and other factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa91a86c-6097-4758-b938-8b9ff6956cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e720f6ef-1469-49b1-97c0-041a242a1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715e9f3-eed6-4e88-9d1c-63a883f9ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path  = '../datasets/walmart_sales.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c5ea7-afe1-49ea-b7b4-105cb7e3ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ac177-ece8-4b50-87aa-620c610b2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3098353-bd6c-4be8-aa3e-718e69487654",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997bee44-75ac-4a24-8822-31e051520219",
   "metadata": {},
   "source": [
    "### Check for nulls - None found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e7cdc6-ed31-44e5-b9b3-cbb034d13299",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2b349-fee1-4f9a-b633-4b8ccebfe7a6",
   "metadata": {},
   "source": [
    "### Let's uniform the columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308b32f9-f729-466c-8d3d-d627b89eff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = list(map(lambda col: col.lower().replace(' ', '_'), df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0bde55-925e-4f23-98b3-11d0cfe3d61e",
   "metadata": {},
   "source": [
    "### General data stats - distributions, scatter matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89176024-4409-4b1f-99f1-077e7e3eb5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec437c-7104-4468-a58f-98416be0a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_fig = make_subplots(rows = 6, cols = 2,\n",
    "                        subplot_titles=(\"Store number\", \"Is holiday?\",\n",
    "                                        \"Weekly sales\",\n",
    "                                        \"Temperature\",\n",
    "                                        \"Fuel price\",\n",
    "                                        \"CPI\",\n",
    "                                        \"Unemployment\"),\n",
    "                        specs = [\n",
    "                            [{}, {}],\n",
    "                            [{\"colspan\": 2}, None],\n",
    "                            [{\"colspan\": 2}, None],\n",
    "                            [{\"colspan\": 2}, None],\n",
    "                            [{\"colspan\": 2}, None],\n",
    "                            [{\"colspan\": 2}, None],\n",
    "                        ],\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6624e-b106-4fae-8c28-8f54f9ddc145",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_fig.add_trace(\n",
    "    go.Histogram(x=df['store']),\n",
    "    row=1, col=1\n",
    ")\n",
    "hist_fig.add_trace(\n",
    "    go.Histogram(x=df['holiday_flag']),\n",
    "    row=1, col=2\n",
    ")\n",
    "hist_fig.add_trace(\n",
    "    go.Histogram(x=df['weekly_sales']),\n",
    "    row=2, col=1\n",
    ")\n",
    "hist_fig.add_trace(\n",
    "    go.Histogram(x=df['temperature']),\n",
    "    row=3, col=1\n",
    ")\n",
    "hist_fig.add_trace(\n",
    "    go.Histogram(x=df['fuel_price']),\n",
    "    row=4, col=1\n",
    ")\n",
    "hist_fig.add_trace(\n",
    "    go.Histogram(x=df['cpi']),\n",
    "    row=5, col=1\n",
    ")\n",
    "hist_fig.add_trace(\n",
    "    go.Histogram(x=df['unemployment']),\n",
    "    row=6, col=1\n",
    ")\n",
    "\n",
    "hist_fig.update_layout(height = 1000, width = 1200)\n",
    "# hist_fig.update_layout(autosize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1520b101-e44d-4c21-8f5f-097a5cc7baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_fig = px.scatter_matrix(df)\n",
    "scatter_fig.update_layout(height = 1200, width = 1200)\n",
    "scatter_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c548aee5-69a3-4a76-88ea-a918dc25617a",
   "metadata": {},
   "source": [
    "It's rather high to make any substantial claims about any factor's relation to our predicted variable (weekly sales). No clear linear trends appear.  \n",
    "Still, the distribution plots appear more promising, with a healthy amount of variance and not a lot of extreme values for  any of the factors.  \n",
    "The only possibly suspicious distribution is that of the consumer price index, with a clear distribution split, implying either a change in how it is calculated at some point in time, a sudden change in the US Dollar's value or some other event, which may not fare well for the model's generalizational skills. Nevertheless, it will initially be considrered as one of the factors to include in the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc114772-80a3-402a-9623-a1fb30444af3",
   "metadata": {},
   "source": [
    "### Datatype cleanup, extra feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb6648-d01b-4d63-9f83-051414e79706",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de721ef9-274c-4d6c-9695-9fb615dc2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], format=\"%d-%m-%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f4488-e681-4526-b2a5-5bacfac8ddcf",
   "metadata": {},
   "source": [
    "### Extra features - week number, month, week of month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240401f-3d12-4d1c-81ee-9bd4f02a6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['week_number'] = df['date'].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ef148-83b4-4539-b2b9-9dd1e27f9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f148a885-ada9-499a-a1fc-fe6dcb645cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['week_of_month'] = df.apply(\n",
    "    lambda row: pendulum.parse(row['date'].strftime('%Y-%m-%d')).week_of_month,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08f273-9310-4e4f-834d-408db8632e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['week_of_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60313622-2f4c-4029-814c-b8b9072d9adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_fig_week = px.scatter_matrix(df,\n",
    "                                 dimensions = ['week_number', 'month', 'week_of_month', 'weekly_sales'])\n",
    "scatter_fig_week.update_layout(height = 1000, width = 1000)\n",
    "scatter_fig_week.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b10268-11e5-4343-8bff-c20bd5098a35",
   "metadata": {},
   "source": [
    "It would seem that typically week 4 is the big spender week in many of the stores. A similar rise can be ovserved in months 11 and 12 (november and december), which include both Thanksgiving Day and Christmas Holidays, which can drive up sales. Worth considering is also the following january fall in sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e5ecc-b1f5-4fe1-9fc5-eca1d41f596a",
   "metadata": {},
   "source": [
    "### Extra features: previous week's sales, temperature, fuel price, did the previous week include a holiday and the differences between current week and last week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29cbe9c-ddcb-4187-8d44-ee859603eb6e",
   "metadata": {},
   "source": [
    "#### Get previous week's values for each store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7c9217-f36d-4d99-84f7-2694ced862ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prev_weekly_sales'] = df.sort_values(['store','date']).groupby(['store'])['weekly_sales'].shift()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfdbc12-8792-4d1c-b8b8-df5e6dee3303",
   "metadata": {},
   "source": [
    "#### Fill first week of data for each store with current value instead of 0 or leaving a NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bbb167-f165-458c-9ccc-f716229b43da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prev_weekly_sales'] = df['prev_weekly_sales'].fillna(df['weekly_sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff466961-f1d3-4c4c-98e0-dfb3ef51c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prev_temperature'] = df.sort_values(['store','date']).groupby(['store'])['temperature'].shift()\n",
    "df['prev_fuel_price'] = df.sort_values(['store','date']).groupby(['store'])['fuel_price'].shift()\n",
    "df['prev_cpi'] = df.sort_values(['store','date']).groupby(['store'])['cpi'].shift()\n",
    "df['prev_unemployment'] = df.sort_values(['store','date']).groupby(['store'])['unemployment'].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b0528d-2d41-467a-a837-f285598f64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prev_temperature'] = df['prev_temperature'].fillna(df['temperature'])\n",
    "df['prev_fuel_price'] = df['prev_fuel_price'].fillna(df['fuel_price'])\n",
    "df['prev_cpi'] = df['prev_cpi'].fillna(df['cpi'])\n",
    "df['prev_unemployment'] = df['prev_unemployment'].fillna(df['unemployment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d65be1-e850-4921-a62f-ad4a79017cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af0ca6b-2a38-46e9-9ab2-fb7b8e7f9a2b",
   "metadata": {},
   "source": [
    "#### Calculate differences from previous week to current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d5493-38df-4d69-80c8-3d4bd04d1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_cols = ['weekly_sales', 'temperature', 'fuel_price', 'cpi', 'unemployment', 'holiday_flag']\n",
    "for col in prev_cols:\n",
    "    df[f'{col}_diff'] = df[f'{col}'] - df[f'prev_{col}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b566e8-aed6-4a49-8c0f-79a9b01495bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prev_holiday_flag'] = df.sort_values(['store','date']).groupby(['store'])['holiday_flag'].shift()\n",
    "df['prev_holiday_flag'] = df['prev_holiday_flag'].fillna(df['holiday_flag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002c59a2-0e00-4bf5-9171-162df595ef16",
   "metadata": {},
   "source": [
    "##### previous month, week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746d595-615a-48f5-8190-f519f03ebc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prev_month'] = df.sort_values(['store','date']).groupby(['store'])['month'].shift()\n",
    "df['prev_month'] = df['prev_month'].fillna(df['month'])\n",
    "df['prev_week_number'] = df.sort_values(['store','date']).groupby(['store'])['week_number'].shift()\n",
    "df['prev_week_number'] = df['prev_week_number'].fillna(df['week_number'])\n",
    "df['prev_week_of_month'] = df.sort_values(['store','date']).groupby(['store'])['week_of_month'].shift()\n",
    "df['prev_week_of_month'] = df['prev_week_of_month'].fillna(df['week_of_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf03482-bd9b-4c28-91b1-5a9eb9e1a387",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f287d3d-3adb-4f9a-b6f5-9a2712901909",
   "metadata": {},
   "source": [
    "### Scatter plot between calculated features and weekly sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13f989-781e-49ff-9171-3516e9227ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_2_cols = [f'prev_{col}' for col in prev_cols]\n",
    "# + [f'{col}_diff' for col in prev_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2bcd0a-baee-46d5-b9f1-fcc69fe471a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_2_cols.append('weekly_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ed9be8-47b7-4a2d-a1fe-b63876bf2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_fig_2 = px.scatter_matrix(df,\n",
    "                                 dimensions = scatter_2_cols)\n",
    "scatter_fig_2.update_layout(height = 1400, width = 1400)\n",
    "scatter_fig_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e7ac21-f0e4-4d78-a650-912ce31c00ef",
   "metadata": {},
   "source": [
    "### Let's try a first quick model, firstly without the extracted shifted / previou week features, then with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388d174-b586-4741-9106-b8aa5a3d0a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21176c-96de-4c0e-a74b-0d3ef45fe99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pure = df[['holiday_flag', 'temperature',\n",
    "       'fuel_price', 'cpi', 'unemployment', 'week_number', 'month',\n",
    "       'week_of_month']]\n",
    "y_pure = df[['weekly_sales']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b70968-52c9-4437-b453-b654f7611855",
   "metadata": {},
   "source": [
    "#### Let's start with simple decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc943ce-ba77-4264-be82-dec3b6c48f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6fdce9-589f-47e5-9791-83ecd57bf6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pure.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c25299d-ff5e-4cf9-8910-a9aa0093355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pure.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839126d4-8afe-408c-ad03-192613d59c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pure_train, X_pure_test, y_pure_train, y_pure_test = train_test_split(X_pure, y_pure, shuffle=True, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4818664f-0845-4c9f-b686-9127b1f9c3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model_pure = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4f220a-8afd-47d1-b96a-aabb78302b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model_pure.fit(X_pure_train, y_pure_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c66b5-5329-40eb-91e6-b499d198d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pure_preds = tree_model_pure.predict(X_pure_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f792085-3810-4ca1-93e0-53a77c550c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pure_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd286f9b-a7c8-4555-a191-aeeedc54777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pure_error = mean_absolute_error(y_pure_test, tree_pure_preds)\n",
    "tree_pure_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6fe3c-37bc-46be-a390-0b2472c34a0e",
   "metadata": {},
   "source": [
    "#### Oof, a mean absolute error of over 440k\n",
    "Let's see how much that actually is, in the context of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7f694-291b-4cc8-bedb-749f2e9b6f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sales = df['weekly_sales'].max()\n",
    "min_sales = df['weekly_sales'].min()\n",
    "avg_sales = df['weekly_sales'].mean()\n",
    "median_sales = df['weekly_sales'].median()\n",
    "sales_data = {'max': [max_sales], 'min': [min_sales], 'avg': [avg_sales], 'median': [median_sales]}\n",
    "sales_df = pd.DataFrame(sales_data, index=['tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e87e6-b8a4-4999-82c6-bc81264608ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df['mae_error'] = round(tree_pure_error,2)\n",
    "sales_df['mae_by_max'] = round(sales_df['mae_error'] * 100 / sales_df['max'],2)\n",
    "sales_df['mae_by_min'] = round(sales_df['mae_error'] * 100 / sales_df['min'],2)\n",
    "sales_df['mae_by_avg'] = round(sales_df['mae_error'] *100 / sales_df['avg'],2)\n",
    "sales_df['mae_by_median'] = round(sales_df['mae_error'] * 100 / sales_df['median'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381952f-5768-4ffb-971f-1ba31d7aeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881938bc-f8f6-45a2-abc6-5c784e67f00f",
   "metadata": {},
   "source": [
    "### Basic decision tree with pure features results:\n",
    "As we can see, the mean error, reach as far as 42% of the mean sales value, making the trained model practically useless\n",
    "But - let's not get discouraged, as this is merely the first, extremely basic model we will look at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c5e1b-c64c-497d-ae8a-4f378da1da68",
   "metadata": {},
   "source": [
    "### Let's explore decision trees further, with different max number of leaf nodes\n",
    "As setting the maximum depth, will stop only at that depth, whilst choosing a maximum number of leaf nodes will try to optimize for best results and potentially drop some branches and reach a further overall depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd50dbb-75af-4418-9b93-92ba8fb40894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd6d97-ad50-4d95-8586-1a55a2694232",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pure_tree_mae = tree_pure_error\n",
    "for max_leaf_nodes in [5, 50, 200, 250, 350, 500, 1000, 1750, 2500, 5000]:\n",
    "    leaf_mae = get_tree_mae(max_leaf_nodes, X_pure_train, X_pure_test, y_pure_train, y_pure_test)\n",
    "    if leaf_mae < best_pure_tree_mae:\n",
    "        best_pure_tree_mae = leaf_mae\n",
    "    print(f'Max leaf nodes: {max_leaf_nodes}, MAE Error: {leaf_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402180b-8fc3-423b-899b-b3981c8047de",
   "metadata": {},
   "source": [
    "#### Under these parameters, the optional number of nodes lies somewhere about **200 max leaf nodes**\n",
    "as such, let's update our results dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d3171-5315-418b-b8ff-09d85f943e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df['mae_error'] = round(best_pure_tree_mae,2)\n",
    "sales_df['mae_by_max'] = round(sales_df['mae_error'] * 100 / sales_df['max'],2)\n",
    "sales_df['mae_by_min'] = round(sales_df['mae_error'] * 100 / sales_df['min'],2)\n",
    "sales_df['mae_by_avg'] = round(sales_df['mae_error'] *100 / sales_df['avg'],2)\n",
    "sales_df['mae_by_median'] = round(sales_df['mae_error'] * 100 / sales_df['median'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7544091-bcb9-4cdc-9bfd-d464d8d0e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea57216-1fb3-4cbf-af68-86613008472d",
   "metadata": {},
   "source": [
    "Still a third of the average sales - quite a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c352eb-a5b7-4786-8d68-64d273f0370b",
   "metadata": {},
   "source": [
    "### Let's look at random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dd54e4-f8c1-49b5-95a2-b2c1853684d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestRegressor()\n",
    "random_forest_model.fit(X_pure_train, y_pure_train.values.ravel())\n",
    "random_forest_preds = random_forest_model.predict(X_pure_test)\n",
    "random_forest_mae = mean_absolute_error(y_pure_test, random_forest_preds)\n",
    "round(random_forest_mae,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6f8b8-7636-41d7-a099-1f9a665cbaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "maes_df = pd.DataFrame({'mae': [round(best_pure_tree_mae, 2)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43486adf-8f58-4c6b-97d1-ebe128b11d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "maes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1303789-648f-4d9a-b18e-e95a8591353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maes_df.index = ['tree_pure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd024466-e9d5-413d-a6f3-a3ee4977379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_df = pd.DataFrame([random_forest_mae], columns=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb20429-ea27-4453-97ab-a1014141ada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_df.index = ['random_forest_pure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ad90a-1b83-4390-bc44-1a690537bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "maes_df = pd.concat([maes_df, random_forest_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e137b5f-cb9a-4c43-b312-5947b4a4f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "maes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9824335-e7b4-410e-a1b6-923bfd5cba52",
   "metadata": {},
   "source": [
    "### With one model clearly better, let's try to explore different dataset feature choices, before going into further model choices or data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d45a09-4c73-48c1-968c-66962edfdabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3042fa-ddba-43c9-95e1-037682da130a",
   "metadata": {},
   "source": [
    "#### Let's first look at very broad feature ranges, potentially removing singular features later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4405f761-b2f2-41ad-bb14-74dc05f19d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the base dataframe information the date\n",
    "df_noeng = df[['store', 'weekly_sales', 'holiday_flag', 'temperature',\n",
    "       'fuel_price', 'cpi', 'unemployment']]\n",
    "\n",
    "# the base dataframe information the date and store\n",
    "df_noeng_nostore = df[['weekly_sales', 'holiday_flag', 'temperature',\n",
    "       'fuel_price', 'cpi', 'unemployment']]\n",
    "\n",
    "# with additional information about week and month\n",
    "df_week = df[['weekly_sales', 'holiday_flag', 'temperature',\n",
    "       'fuel_price', 'cpi', 'unemployment', 'week_number', 'month',\n",
    "       'week_of_month']]\n",
    "\n",
    "# with additional information about week and month AND data about the previous week\n",
    "df_week_prev = df[['weekly_sales', 'holiday_flag', 'temperature',\n",
    "       'fuel_price', 'cpi', 'unemployment', 'week_number', 'month',\n",
    "       'week_of_month', 'prev_weekly_sales', 'prev_temperature',\n",
    "       'prev_fuel_price', 'prev_cpi', 'prev_unemployment', 'prev_month',\n",
    "       'prev_week_number', 'prev_week_of_month']]\n",
    "\n",
    "df_week_prev_diff = df[['weekly_sales', 'holiday_flag', 'temperature',\n",
    "       'fuel_price', 'cpi', 'unemployment', 'week_number', 'month',\n",
    "       'week_of_month', 'prev_weekly_sales', 'prev_temperature',\n",
    "       'prev_fuel_price', 'prev_cpi', 'prev_unemployment', 'weekly_sales_diff',\n",
    "       'temperature_diff', 'fuel_price_diff', 'cpi_diff', 'unemployment_diff',\n",
    "       'prev_holiday_flag', 'holiday_flag_diff', 'prev_month',\n",
    "       'prev_week_number', 'prev_week_of_month']]\n",
    "\n",
    "df_week_prev_diff_store = df[['store', 'weekly_sales', 'holiday_flag', 'temperature',\n",
    "       'fuel_price', 'cpi', 'unemployment', 'week_number', 'month',\n",
    "       'week_of_month', 'prev_weekly_sales', 'prev_temperature',\n",
    "       'prev_fuel_price', 'prev_cpi', 'prev_unemployment', 'weekly_sales_diff',\n",
    "       'temperature_diff', 'fuel_price_diff', 'cpi_diff', 'unemployment_diff',\n",
    "       'prev_holiday_flag', 'holiday_flag_diff', 'prev_month',\n",
    "       'prev_week_number', 'prev_week_of_month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917144db-4a91-410c-a804-259feffaccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [df_noeng, df_noeng_nostore, df_week, df_week_prev, df_week_prev_diff, df_week_prev_diff_store]\n",
    "dataset_names = ['noeng', 'noeng_nostore', 'week', 'week_prev', 'week_prev_diff', 'week_prev_diff_store']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf5f9a-3cca-49c6-9559-615bc9a90903",
   "metadata": {},
   "source": [
    "### Function to assess datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d76a1b5-d82d-46e0-982b-0a209c5cba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_sales_dataset_tree(input_df):\n",
    "    input_X = input_df.drop(['weekly_sales'], axis=1)\n",
    "    input_y = input_df[['weekly_sales']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_X, input_y, shuffle=True, train_size=0.8)\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes = 200)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179bd4d7-4d89-4a7a-9b41-c4a2fd3725f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what_mae = assess_sales_dataset_tree(df_noeng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00911aa-d60f-46d2-a1c2-cfb96c0d6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201be394-5b16-434f-b324-a7ceeae357e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_tree_dataset_mae = df[['weekly_sales']].max().values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071039b3-160a-416d-a53a-8a29bf0d49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_tree_dataset_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78882c92-21cd-4342-b4ac-7d7ce2ff6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree_dataset_mae = df[['weekly_sales']].max().values[0]\n",
    "for df_index, dataset in enumerate(datasets):\n",
    "    df_mae = assess_sales_dataset_tree(dataset)\n",
    "    if df_mae < best_tree_dataset_mae:\n",
    "        best_tree_dataset_mae = df_mae\n",
    "    print(f'df: {dataset_names[df_index]}, MAE Error: {df_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9825b7fe-091a-4e61-afac-e404fcf356d1",
   "metadata": {},
   "source": [
    "### Clearly, using the previous week information yields the best results\n",
    "* What's interesting is how the removal of the 'store' information, had terrible consequences. Perhaps it is unsurprising - information about which store you are considering will probably strongly influence the sales prediction, as can be seen in the weekly sales distribution, which has quite a large range.\n",
    "* The question is - do we want to use this information, for most accurate results or do we want to ignore it, to make the model more general and applicable to any store not present in the dataset, but rather limiting ourselves to only data about the region such potential store would reside in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38c03a-57a4-4d85-9475-cbc230248f18",
   "metadata": {},
   "source": [
    "### BUT HOLD ON - we can't use the _diff columns! Data leakage!\n",
    "* To calculate the difference between current week's values and previous week's values - you need CURRENT WEEK'S VALUES!!!!\n",
    "* Which might be fine, if you consider current week's values for the prediction factors, but you absolutely CANNOT use weekly_sales_diff as a input factor\n",
    "* It might be interesting to consider predicting without definitive differences (as in future-oriented) or with current values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b6695-662f-41fe-b0dc-8fb753719c4c",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "wow, even better - and with no parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e0b71-3618-4283-bf30-3b130cbccd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "input_X = df_week_prev_diff.drop(['weekly_sales', 'weekly_sales_diff'], axis=1)\n",
    "input_y = df_week_prev_diff[['weekly_sales']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_X, input_y, shuffle=True, train_size=0.8)\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "xgb_mae = mean_absolute_error(xgb_preds, y_test)\n",
    "xgb_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32bf44-c8ee-4cac-b23b-29b2cc2ceaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgb_mae(n_estimators, learning_rate, train_X, val_X, train_y, val_y, early_stopping_rounds = 5):\n",
    "    model = XGBRegressor(n_estimators = n_estimators, learning_rate=learning_rate)\n",
    "    model.fit(train_X, train_y,\n",
    "             early_stopping_rounds=early_stopping_rounds,\n",
    "             eval_set=[(val_X, val_y)],\n",
    "             verbose=False)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eef347-9d17-4785-acf9-ef6a7b148eca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_xgb_mae = y_train.max().values[0]\n",
    "for n_estimators in [100, 250, 500, 1000, 2000, 5000]:\n",
    "    for learning_rate in [0.001, 0.01, 0.02, 0.05, 0.1]:\n",
    "        xgb_mae = get_xgb_mae(n_estimators, learning_rate, X_train, X_test, y_train, y_test)\n",
    "        if xgb_mae < best_xgb_mae:\n",
    "            best_xgb_mae = xgb_mae\n",
    "        print(f'n_estimators: {n_estimators}, learning_rate: {learning_rate}, MAE Error: {xgb_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a250d1-3d25-4a76-bf1d-20054cf2151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfb9cee-49cb-42ea-a853-7fbf8278cf6c",
   "metadata": {},
   "source": [
    "#### Let's say the best parameters were n_estimators 2000, lr = 0.01\n",
    "* n_estimators: 1000, learning_rate: 0.01, MAE Error: 9856.41850281663\n",
    "* n_estimators: 2000, learning_rate: 0.01, MAE Error: 9853.203303710181\n",
    "* n_estimators: 5000, learning_rate: 0.01, MAE Error: 9853.203303710181"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c1a57-1f4e-46ab-b88b-fe08bcbcd387",
   "metadata": {},
   "source": [
    "### SVR, SGDRegressor and good ol' LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f137294-3d0d-4df8-aa5a-002e2a76b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr_model = SVR()\n",
    "svr_model.fit(X_train, y_train.values.ravel())\n",
    "svr_preds = svr_model.predict(X_test)\n",
    "svr_mae = mean_absolute_error(svr_preds, y_test)\n",
    "svr_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bbb6f7-5507-45fe-bd42-ff6cd99bec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "svr_model = LinearRegression()\n",
    "svr_model.fit(X_train, y_train.values.ravel())\n",
    "svr_preds = svr_model.predict(X_test)\n",
    "svr_mae = mean_absolute_error(svr_preds, y_test)\n",
    "svr_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a977d09c-a517-4b9f-b3ed-ac02ce41c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "svr_model = SGDRegressor()\n",
    "svr_model.fit(X_train, y_train.values.ravel())\n",
    "svr_preds = svr_model.predict(X_test)\n",
    "svr_mae = mean_absolute_error(svr_preds, y_test)\n",
    "svr_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d492fb65-102e-4286-864f-06f7a2ea0666",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "while for every experiment we often shuffled the dataset, and so we probably should have a good idea of how the models perform on different samples from, the data, a uniform approach is useful for all cases, therefore shall be developed.  \n",
    "Plus, the dataset is quite small, so that it shouldn't impact the testing time too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd927ba-2483-4d67-94db-3e02a24f1480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b6c02-56fe-473f-b676-24b9d1a5977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_model = XGBRegressor(n_estimators = 2000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc57fa-999c-4fa5-ac66-726d5b0e3c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = -1 * cross_val_score(best_xgb_model,\n",
    "                              input_X, input_y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2012a303-3114-4ffd-b916-bd872081adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d50f6f-50f6-4362-a7e0-ada8f1fece5e",
   "metadata": {},
   "source": [
    "Clearly, there is some variance in the model's performance, based on the data sample - for one of the samples - it even performed better than our previous estimation, on average though, while validating on the whole dataset, it performed a little worse. It's worth being aware of the worst case scenarios / the range of performance for any model, to accurately judge it's usefullness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1a352-eefa-4fc2-b1f7-46b7d244b8fe",
   "metadata": {},
   "source": [
    "### Scaling factors values / uniforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c94bd-5f4b-415d-84a0-3e74721e8d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73076e1-6010-41b0-87d4-8e485cfc0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def preprocess_and_train_regression(model, X_train, y_train, X_valid, y_valid):\n",
    "    # Preprocessing transformers\n",
    "    numerical_transformer = StandardScaler()\n",
    "\n",
    "    numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64', 'UInt32', 'int32']]\n",
    "    \n",
    "    # combined preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers = [\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            # ('cat', categorical_transformer, categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Bundle preprocessing and modeling code in a pipeline\n",
    "    train_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Preprocessing of training data, fit model \n",
    "    train_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Preprocessing of validation data, get predictions\n",
    "    preds = train_pipeline.predict(X_valid)\n",
    "    \n",
    "    mae = mean_absolute_error(y_valid, preds)\n",
    "    r2 = r2_score(y_valid, preds)\n",
    "    return mae, r2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e3784-1069-4827-9606-7935eba2421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, r2, model = preprocess_and_train_regression(xgb_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808af51d-5a5e-43c4-8fd3-1ac54aa08ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd48472-0185-425c-8d42-e7a970620177",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500b166-ecc3-4201-a529-a9dc5beab73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c719497-df4f-4d08-847c-c4e16124c8d5",
   "metadata": {},
   "source": [
    "### mae ~47k - pretty good - with standard scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea352cd8-d1c6-44df-90e3-a6cf687dac2d",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "* https://www.kaggle.com/code/ryanholbrook/mutual-information - mutual information scores  \n",
    "Create functions to:\n",
    "* take in model, dataset, some other parameters then:\n",
    "* scale /preprocess the data\n",
    "* train model\n",
    "* crossvalidate\n",
    "* return results of mae, r2 score, median_error? as a new row of dataframe collecting results from all types of models, datasets etc\n",
    "* for each model calculate the proportion of error to the max, min, average and median prices\n",
    "* try to get separate results for each store? aka for which store do we perform the worse\n",
    "* generally explore other metrics. Are there training curves available etc?  \n",
    "Then:\n",
    "* Wrap it all in the initial EDA exploration, with the conclusions and choose the best model (XGB) with the best observed results, explain why and how it uses previous week's values etc\n",
    "* but also note what NOT to do (weekly_sales_diff) and the general process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a85ff-1292-4c9b-b160-80d07994f236",
   "metadata": {},
   "source": [
    "### Store classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc667606-3b94-4439-ab5a-7df817d37871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics  import f1_score,accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8410083-6159-4c49-bb9d-a1de078b9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_train_classification(model, dataset, target_col):\n",
    "\n",
    "    X = dataset.drop([target_col], axis=1)\n",
    "    y = dataset[[target_col]]\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y.values.ravel(), shuffle=True, train_size=0.8)\n",
    "    \n",
    "    # Preprocessing transformers\n",
    "    numerical_transformer = StandardScaler()\n",
    "\n",
    "    numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64', 'UInt32', 'int32']]\n",
    "    \n",
    "    # combined preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers = [\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            # ('cat', categorical_transformer, categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Bundle preprocessing and modeling code in a pipeline\n",
    "    train_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Preprocessing of training data, fit model \n",
    "    train_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Preprocessing of validation data, get predictions\n",
    "    preds = train_pipeline.predict(X_valid)\n",
    "    \n",
    "    acc = accuracy_score(y_valid, preds)\n",
    "    f1 = f1_score(y_valid, preds, average=None, labels=model.classes_)\n",
    "    cm = confusion_matrix(y_valid, preds, labels=model.classes_)\n",
    "    labels = model.classes_\n",
    "\n",
    "    return acc, f1, cm, labels, model, preds, X_valid, y_valid, train_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c553c5-bcc0-478d-8cb0-997563ca2cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(cm, labels):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix = cm,\n",
    "                             display_labels=labels\n",
    "                             )\n",
    "    disp_plot = disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f7baa-c879-40b2-bf1e-dfe8e381db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with additional information about week and month\n",
    "df_week_store = df[['store', 'weekly_sales', 'holiday_flag', 'temperature',\n",
    "       'fuel_price', 'cpi', 'unemployment', 'week_number', 'month',\n",
    "       'week_of_month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6954c53-392f-4a0d-acb1-0ebd4d5a3319",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde4533-f146-499e-882e-56b23ef71f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, f1, cm, labels, model, preds, X_valid, y_valid, train_pipeline = preprocess_and_train_classification(model = svc_model,\n",
    "                                                                                                          dataset = df_week_store,\n",
    "                                                                                                          target_col = 'store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac0bed-3ab8-4d27-bf05-93c3f8f493e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = dataset.drop([target_col], axis=1)\n",
    "# y = dataset[[target_col]]\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, shuffle=True, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6ae48-6875-4db7-9cd1-6ffaffc5e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(cm, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee5b21-498d-4b2c-8f77-78e16774cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2383ff1a-723c-4d17-9e88-f349ea7ea74f",
   "metadata": {},
   "source": [
    "### 75% accuracy prediction of store - not terrible\n",
    "let's see the results with the \"prev\" and \"diff\" columns - why not - mroe data, even though partially reduntant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4bf75-c867-45b4-aafd-712269bc2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_week_prev_diff_store = df[['store', 'weekly_sales', 'holiday_flag', 'temperature',\n",
    "       'fuel_price', 'cpi', 'unemployment', 'week_number', 'month',\n",
    "       'week_of_month', 'prev_weekly_sales', 'prev_temperature',\n",
    "       'prev_fuel_price', 'prev_cpi', 'prev_unemployment', 'weekly_sales_diff',\n",
    "       'temperature_diff', 'fuel_price_diff', 'cpi_diff', 'unemployment_diff',\n",
    "       'prev_holiday_flag', 'holiday_flag_diff', 'prev_month',\n",
    "       'prev_week_number', 'prev_week_of_month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711f492-9c24-4ace-b358-5c9c1990f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, f1, cm, labels, model, preds, X_valid, y_valid, train_pipeline = preprocess_and_train_classification(model = svc_model,\n",
    "                                                                                                          dataset = df_week_prev_diff_store,\n",
    "                                                                                                          target_col = 'store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe1ed6-3c42-4702-86db-af8db5b9e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6237671d-ee98-439f-b728-7aeb52901c5d",
   "metadata": {},
   "source": [
    "#### Welp - a reduction in accuracy - maybe a different model could help though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b89ef-afaf-4cd6-a25b-54805b1adc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af344f-f6d6-460b-87eb-c0d8b1b98c8d",
   "metadata": {},
   "source": [
    "## Important note - dataframe with all engineered features saved to file 'walmart_sales_engineered.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c254de4-7b06-4ff9-9ac3-ff2533f18d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../datasets/walmart_sales_engineered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfa554-5f63-4b2c-b5a8-68658fbaf880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_zoo_venv",
   "language": "python",
   "name": "ml_zoo_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
